{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5):\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "\n",
    "def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features, f_idxs,idxs,depth=10, min_leaf=5):\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        #Will make it recursive later\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "       \n",
    "\n",
    "    def find_better_split(self, var_idx):\n",
    "        #Lets write it later\n",
    "        pass\n",
    "    \n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0 \n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2)\n",
    " \n",
    " def find_better_split(self, var_idx):\n",
    "        x, y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y,sort_x = y[sort_idx], x[sort_idx]\n",
    "        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n",
    "        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_varsplit(self):\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x<=self.split)[0]\n",
    "        rhs = np.nonzero(x>self.split)[0]\n",
    "        lf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        rf_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        self.lhs = DecisionTree(self.x, self.y, self.n_features, lf_idxs, self.idxs[lhs], depth=self.depth-1, min_leaf=self.min_leaf)\n",
    "        self.rhs = DecisionTree(self.x, self.y, self.n_features, rf_idxs, self.idxs[rhs], depth=self.depth-1, min_leaf=self.min_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import scipy.stats as sps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r'C:\\Users\\krish\\Desktop\\Subjects\\AppliedML\\Lec-4\\JPM-HistData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['stock_return'] = dataset['Adj Close'].pct_change()\n",
    "dataset = dataset[dataset['stock_return'].notna()]\n",
    "dataset = dataset[dataset.stock_return != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>stock_return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>5.074074</td>\n",
       "      <td>0.292716</td>\n",
       "      <td>63900</td>\n",
       "      <td>0.007354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.148148</td>\n",
       "      <td>0.296989</td>\n",
       "      <td>40500</td>\n",
       "      <td>0.014598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.148148</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>0.294852</td>\n",
       "      <td>18900</td>\n",
       "      <td>-0.007196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.301262</td>\n",
       "      <td>97200</td>\n",
       "      <td>0.021740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1980-03-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.185185</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>0.293784</td>\n",
       "      <td>90300</td>\n",
       "      <td>-0.024822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open      High       Low     Close  Adj Close  Volume  \\\n",
       "1  1980-03-18   0.0  5.111111  5.037037  5.074074   0.292716   63900   \n",
       "2  1980-03-19   0.0  5.166667  5.111111  5.148148   0.296989   40500   \n",
       "3  1980-03-20   0.0  5.148148  5.092593  5.111111   0.294852   18900   \n",
       "4  1980-03-21   0.0  5.222222  5.111111  5.222222   0.301262   97200   \n",
       "5  1980-03-24   0.0  5.185185  5.092593  5.092593   0.293784   90300   \n",
       "\n",
       "   stock_return  \n",
       "1      0.007354  \n",
       "2      0.014598  \n",
       "3     -0.007196  \n",
       "4      0.021740  \n",
       "5     -0.024822  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns = ['Date','Open','High','Low','Close','Adj Close','Volume','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1980-03-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.037037</td>\n",
       "      <td>5.074074</td>\n",
       "      <td>0.292716</td>\n",
       "      <td>63900</td>\n",
       "      <td>0.007354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980-03-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.166667</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.148148</td>\n",
       "      <td>0.296989</td>\n",
       "      <td>40500</td>\n",
       "      <td>0.014598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980-03-20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.148148</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>0.294852</td>\n",
       "      <td>18900</td>\n",
       "      <td>-0.007196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980-03-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>0.301262</td>\n",
       "      <td>97200</td>\n",
       "      <td>0.021740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1980-03-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.185185</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>5.092593</td>\n",
       "      <td>0.293784</td>\n",
       "      <td>90300</td>\n",
       "      <td>-0.024822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Open      High       Low     Close  Adj Close  Volume    target\n",
       "1  1980-03-18   0.0  5.111111  5.037037  5.074074   0.292716   63900  0.007354\n",
       "2  1980-03-19   0.0  5.166667  5.111111  5.148148   0.296989   40500  0.014598\n",
       "3  1980-03-20   0.0  5.148148  5.092593  5.111111   0.294852   18900 -0.007196\n",
       "4  1980-03-21   0.0  5.222222  5.111111  5.222222   0.301262   97200  0.021740\n",
       "5  1980-03-24   0.0  5.185185  5.092593  5.092593   0.293784   90300 -0.024822"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target_col):\n",
    "    elements,counts = np.unique(target_col,return_counts = True)\n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfoGain(data,split_attribute_name,target_name=\"target\"):\n",
    "    \n",
    "    #Calculate the entropy of the total dataset\n",
    "    total_entropy = entropy(data[target_name])\n",
    "    \n",
    "    ##Calculate the entropy of the dataset\n",
    "    \n",
    "    #Calculate the values and the corresponding counts for the split attribute \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n",
    "    \n",
    "    #Calculate the weighted entropy\n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n",
    "    \n",
    "    #Calculate the information gain\n",
    "    Information_Gain = total_entropy - Weighted_Entropy\n",
    "    return Information_Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data,originaldata,features,target_attribute_name=\"target\",parent_node_class = None):\n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n",
    "    \n",
    "    #If all target_values have the same value, return this value\n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:\n",
    "        return np.unique(data[target_attribute_name])[0]\n",
    "    \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset\n",
    "    elif len(data)==0:\n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n",
    "    \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n",
    "    #the mode target feature value is stored in the parent_node_class variable.\n",
    "    \n",
    "    elif len(features) ==0:\n",
    "        return parent_node_class\n",
    "    \n",
    "    #If none of the above holds true, grow the tree!\n",
    "    \n",
    "    else:\n",
    "        #Set the default value for this node --> The mode target feature value of the current node\n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n",
    "        \n",
    "        \n",
    "        ################################################################################################################\n",
    "        ############!!!!!!!!!Implement the subspace sampling. Draw a number of m = sqrt(p) features!!!!!!!!#############\n",
    "        ###############################################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        features = np.random.choice(features,size=np.int(np.sqrt(len(features))),replace=False)\n",
    "        \n",
    "        #Select the feature which best splits the dataset\n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n",
    "        best_feature_index = np.argmax(item_values)\n",
    "        best_feature = features[best_feature_index]\n",
    "        \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n",
    "        #gain in the first run\n",
    "        tree = {best_feature:{}}\n",
    "        \n",
    "        #Remove the feature with the best inforamtion gain from the feature space\n",
    "        features = [i for i in features if i != best_feature]\n",
    "        \n",
    "        \n",
    "        #Grow a branch under the root node for each possible value of the root node feature\n",
    "        \n",
    "        for value in np.unique(data[best_feature]):\n",
    "            value = value\n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n",
    "            sub_data = data.where(data[best_feature] == value).dropna()\n",
    "            \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)\n",
    "            \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n",
    "            tree[best_feature][value] = subtree\n",
    "            \n",
    "        return(tree)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(query,tree,default = 'p'):\n",
    "        \n",
    "    for key in list(query.keys()):\n",
    "        if key in list(tree.keys()):\n",
    "            try:\n",
    "                result = tree[key][query[key]] \n",
    "            except:\n",
    "                return default\n",
    "            result = tree[key][query[key]]\n",
    "            if isinstance(result,dict):\n",
    "                return predict(query,result)\n",
    "\n",
    "            else:\n",
    "                return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset):\n",
    "    training_data = dataset.iloc[:round(0.75*len(dataset))].reset_index(drop=True)#We drop the index respectively relabel the index\n",
    "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes\n",
    "    testing_data = dataset.iloc[round(0.75*len(dataset)):].reset_index(drop=True)\n",
    "    return training_data,testing_data\n",
    "\n",
    "\n",
    "training_data = train_test_split(dataset)[0]\n",
    "testing_data = train_test_split(dataset)[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomForest_Train(dataset,number_of_Trees):\n",
    "    #Create a list in which the single forests are stored\n",
    "    random_forest_sub_tree = []\n",
    "    \n",
    "    #Create a number of n models\n",
    "    for i in range(number_of_Trees):\n",
    "        #Create a number of bootstrap sampled datasets from the original dataset \n",
    "        bootstrap_sample = dataset.sample(frac=1,replace=True)\n",
    "        \n",
    "        #Create a training and a testing datset by calling the train_test_split function\n",
    "        bootstrap_training_data = train_test_split(bootstrap_sample)[0]\n",
    "        bootstrap_testing_data = train_test_split(bootstrap_sample)[1] \n",
    "        \n",
    "        \n",
    "        #Grow a tree model for each of the training data\n",
    "        #We implement the subspace sampling in the ID3 algorithm itself. Hence take a look at the ID3 algorithm above!\n",
    "        random_forest_sub_tree.append(ID3(bootstrap_training_data,bootstrap_training_data,bootstrap_training_data.drop(labels=['target'],axis=1).columns))\n",
    "        \n",
    "    return random_forest_sub_tree\n",
    "\n",
    "\n",
    "        \n",
    "random_forest = RandomForest_Train(dataset,50)\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target:  2010-09-29\n",
      "prediction:  -0.013863727091326217\n"
     ]
    }
   ],
   "source": [
    "#######Predict a new query instance###########\n",
    "def RandomForest_Predict(query,random_forest,default='p'):\n",
    "    predictions = []\n",
    "    for tree in random_forest:\n",
    "        predictions.append(predict(query,tree,default))\n",
    "    return sps.mode(predictions)[0][0]\n",
    "\n",
    "\n",
    "query = testing_data.iloc[0,:].drop('target').to_dict()\n",
    "query_target = testing_data.iloc[0,0]\n",
    "print('target: ',query_target)\n",
    "prediction = RandomForest_Predict(query,random_forest)\n",
    "print('prediction: ',prediction)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  0.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#######Test the model on the testing data and return the accuracy###########\n",
    "def RandomForest_Test(data,random_forest):\n",
    "    data['predictions'] = None\n",
    "    for i in range(len(data)):\n",
    "        query = data.iloc[i,:].drop('target').to_dict()\n",
    "        data.loc[i,'predictions'] = RandomForest_Predict(query,random_forest,default='p')\n",
    "    accuracy = sum(data['predictions'] == data['target'])/len(data)*100\n",
    "    print('The prediction accuracy is: ',sum(data['predictions'] == data['target'])/len(data)*100,'%')\n",
    "    return accuracy\n",
    "\n",
    "RandomForest_Test(testing_data,random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  54.886597938144334 %\n",
      "The prediction accuracy is:  29.60824742268041 %\n",
      "The prediction accuracy is:  17.608247422680414 %\n",
      "The prediction accuracy is:  7.587628865979382 %\n",
      "The prediction accuracy is:  3.6288659793814433 %\n",
      "The prediction accuracy is:  2.7216494845360826 %\n",
      "The prediction accuracy is:  1.6907216494845363 %\n",
      "The prediction accuracy is:  1.0309278350515463 %\n",
      "The prediction accuracy is:  0.6597938144329897 %\n",
      "The prediction accuracy is:  0.2061855670103093 %\n",
      "The prediction accuracy is:  0.2061855670103093 %\n",
      "The prediction accuracy is:  0.0 %\n",
      "The prediction accuracy is:  0.0 %\n",
      "The prediction accuracy is:  0.0 %\n",
      "The prediction accuracy is:  0.0 %\n",
      "The prediction accuracy is:  0.0 %\n",
      "The prediction accuracy is:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "ax0 = fig.add_subplot(111)\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for i in range(1,11,1):\n",
    "    random_forest = RandomForest_Train(dataset,i)\n",
    "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
    "\n",
    "for i in range(10,110,10):\n",
    "    random_forest = RandomForest_Train(dataset,i)\n",
    "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
    "\n",
    "for i in range(100,1100,100):\n",
    "    random_forest = RandomForest_Train(dataset,i)\n",
    "    accuracy.append(RandomForest_Test(testing_data,random_forest))\n",
    "\n",
    "print(accuracy)\n",
    "ax0.plot(np.logspace(0,3,30),accuracy)\n",
    "ax0.set_yticks(np.linspace(50,100,50))\n",
    "ax0.set_title(\"Accuracy with respect to the numer of trees in the random forest\")\n",
    "ax0.set_xscale('log')\n",
    "ax0.set_xlabel(\"Number of Trees\")\n",
    "ax0.set_ylabel('Accuracy(%)')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
